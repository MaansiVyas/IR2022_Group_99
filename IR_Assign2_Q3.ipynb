{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 341,
      "id": "65d41299",
      "metadata": {
        "id": "65d41299"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 342,
      "id": "18679723",
      "metadata": {
        "id": "18679723"
      },
      "outputs": [],
      "source": [
        "# 5 Classes\n",
        "mapping = {\n",
        "    'comp.graphics': 0,\n",
        "    'sci.med': 1,\n",
        "    'talk.politics.misc': 2,\n",
        "    'rec.sport.hockey': 3,\n",
        "    'sci.space': 4\n",
        "}\n",
        "\n",
        "reverseMapping = {\n",
        "    0: 'comp.graphics',\n",
        "    1: 'sci.med',\n",
        "    2: 'talk.politics.misc',\n",
        "    3: 'rec.sport.hockey',\n",
        "    4: 'sci.space'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pp8vZOVfLbbn",
        "outputId": "fb834b3f-fdd7-4a43-8b6e-60ff65673e48"
      },
      "id": "pp8vZOVfLbbn",
      "execution_count": 343,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 344,
      "id": "c61e6e58",
      "metadata": {
        "id": "c61e6e58"
      },
      "outputs": [],
      "source": [
        "data_file = []\n",
        "target = []\n",
        "path = \"/content/drive/MyDrive/20_newsgroups/\"\n",
        "for dir in os.listdir(path):\n",
        "    if dir in mapping:\n",
        "             \n",
        "        for file in os.listdir(path+dir+\"/\"):\n",
        "            try:\n",
        "               \n",
        "                f = open(path+dir+\"/\"+file)\n",
        "                data_file.append(f.read())\n",
        "                target.append(mapping[dir])\n",
        "            except:\n",
        "             \n",
        "                f = open(path+dir+\"/\"+file,\"rb\")\n",
        "                data_file.append(f.read().decode('utf-8', 'backslashreplace'))\n",
        "                target.append(mapping[dir])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_file = np.array(data_file)\n",
        "target = np.array(target)"
      ],
      "metadata": {
        "id": "EjDzG6pzOBC6"
      },
      "id": "EjDzG6pzOBC6",
      "execution_count": 345,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "stopWords = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcvOoehYMpR8",
        "outputId": "7c4ec303-411d-4707-ba4d-767676246b8b"
      },
      "id": "wcvOoehYMpR8",
      "execution_count": 346,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method for data Pre-processing"
      ],
      "metadata": {
        "id": "N9bkkex0Oj6k"
      },
      "id": "N9bkkex0Oj6k"
    },
    {
      "cell_type": "code",
      "execution_count": 347,
      "metadata": {
        "id": "l7LaVBnz9BtD"
      },
      "outputs": [],
      "source": [
        "def remove_numbers(txt_data):\n",
        "   numbers=\"0123456789\"\n",
        "   for i in range(len(numbers)):\n",
        "       txt_data=np.char.replace(txt_data, numbers[i], ' ')\n",
        "       txt_data = np.char.replace(txt_data, \"  \", \" \")\n",
        "   return txt_data"
      ],
      "id": "l7LaVBnz9BtD"
    },
    {
      "cell_type": "code",
      "execution_count": 348,
      "metadata": {
        "id": "ozLuhoX4FMio"
      },
      "outputs": [],
      "source": [
        "def punctuations_removal_from_data(txt_data):\n",
        "    sym = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
        "    for i in range(len(sym)):\n",
        "        txt_data = np.char.replace(txt_data, sym[i], ' ')\n",
        "        txt_data = np.char.replace(txt_data, \"  \", \" \")\n",
        "    txt_data = np.char.replace(txt_data, ',', ' ')\n",
        "    txt_data = np.char.replace(txt_data, \"'\", \"\")#additionally removing apostrophe\n",
        "    return txt_data"
      ],
      "id": "ozLuhoX4FMio"
    },
    {
      "cell_type": "code",
      "execution_count": 349,
      "metadata": {
        "id": "hDYjWX99HLjp"
      },
      "outputs": [],
      "source": [
        "def to_lower_case(txt_data):\n",
        "    return np.char.lower(txt_data)"
      ],
      "id": "hDYjWX99HLjp"
    },
    {
      "cell_type": "code",
      "execution_count": 350,
      "metadata": {
        "id": "FS4ezTSsHywy"
      },
      "outputs": [],
      "source": [
        "def stop_words_removal_from_data(txt_data):\n",
        "    stopWords = stopwords.words('english')\n",
        "    words = word_tokenize(str(txt_data))\n",
        "    clean_data = \"\"\n",
        "    for w in words:\n",
        "        if w not in stopWords:\n",
        "            clean_data = clean_data + \" \" + w\n",
        "    return np.char.strip(clean_data)"
      ],
      "id": "FS4ezTSsHywy"
    },
    {
      "cell_type": "code",
      "execution_count": 351,
      "metadata": {
        "id": "gevpVUTqIsLD"
      },
      "outputs": [],
      "source": [
        "def short_char_removal(txt_data):\n",
        "    words = word_tokenize(str(txt_data))\n",
        "    clean_data = \"\"\n",
        "    for w in words:\n",
        "        if len(w) > 1:\n",
        "            clean_data = clean_data + \" \" + w\n",
        "    return np.char.strip(clean_data)"
      ],
      "id": "gevpVUTqIsLD"
    },
    {
      "cell_type": "code",
      "execution_count": 352,
      "metadata": {
        "id": "K-qfZWoTlY79"
      },
      "outputs": [],
      "source": [
        "def lemmatize_text(txt_data):\n",
        "    lemmatizer = WordNetLemmatizer()   \n",
        "    tokens = word_tokenize(str(txt_data))\n",
        "    clean_data = \"\"\n",
        "    for w in tokens:\n",
        "        clean_data = clean_data + \" \" +lemmatizer.lemmatize(w)\n",
        "    return np.char.strip(clean_data)"
      ],
      "id": "K-qfZWoTlY79"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method for data Pre processing"
      ],
      "metadata": {
        "id": "P8CwOChPzD3N"
      },
      "id": "P8CwOChPzD3N"
    },
    {
      "cell_type": "code",
      "execution_count": 353,
      "metadata": {
        "id": "5eRCSbOsKs0G"
      },
      "outputs": [],
      "source": [
        "def process_txt_data(txt_data):\n",
        "  txt_data=remove_numbers(txt_data)\n",
        "  txt_data=punctuations_removal_from_data(txt_data)\n",
        "  txt_data=to_lower_case(txt_data)\n",
        "  txt_data=stop_words_removal_from_data(txt_data)\n",
        "  txt_data=short_char_removal(txt_data)\n",
        "  txt_data=lemmatize_text(txt_data)\n",
        "  return txt_data"
      ],
      "id": "5eRCSbOsKs0G"
    },
    {
      "cell_type": "code",
      "execution_count": 354,
      "id": "9e63cc17",
      "metadata": {
        "id": "9e63cc17"
      },
      "outputs": [],
      "source": [
        "for fileNo,file in enumerate(data_file):\n",
        "    data_file[fileNo] = process_txt_data(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method to split data"
      ],
      "metadata": {
        "id": "K1kH5fS7O_ph"
      },
      "id": "K1kH5fS7O_ph"
    },
    {
      "cell_type": "code",
      "execution_count": 355,
      "id": "b81d20fd",
      "metadata": {
        "id": "b81d20fd"
      },
      "outputs": [],
      "source": [
        "def methodRandomSplitData(splitRatio = 0.8):\n",
        "    dataSize = data_file.shape[0]\n",
        "    trainingSize = int(dataSize * splitRatio)   \n",
        "    randomIndexes = np.random.choice(dataSize, size=trainingSize)\n",
        "    #test data\n",
        "    testX = data_file[np.setdiff1d(range(dataSize), randomIndexes)]\n",
        "    testY = target[np.setdiff1d(range(dataSize), randomIndexes)]\n",
        "    #train data.\n",
        "    trainX = data_file[randomIndexes]\n",
        "    trainY = target[randomIndexes] \n",
        "    return (trainX, trainY, testX, testY)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting data into training and test"
      ],
      "metadata": {
        "id": "gYhRdgF2QHGJ"
      },
      "id": "gYhRdgF2QHGJ"
    },
    {
      "cell_type": "code",
      "execution_count": 356,
      "id": "7f99a65b",
      "metadata": {
        "id": "7f99a65b"
      },
      "outputs": [],
      "source": [
        "train, trainLabel, test, testLabel = methodRandomSplitData()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d115551c",
      "metadata": {
        "id": "d115551c"
      },
      "source": [
        "TF-ICF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 357,
      "id": "3b278b60",
      "metadata": {
        "id": "3b278b60"
      },
      "outputs": [],
      "source": [
        "ClassFrequency = {}\n",
        "for indexNo,text  in enumerate(train):\n",
        "    for w in text.split(\" \"):\n",
        "        if w not in ClassFrequency:\n",
        "            ClassFrequency[w] = set()\n",
        "        ClassFrequency[w].add(target[indexNo])\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 358,
      "id": "414357da",
      "metadata": {
        "id": "414357da"
      },
      "outputs": [],
      "source": [
        "InverseClassFrequency = {}\n",
        "for w in ClassFrequency.keys():\n",
        "    InverseClassFrequency[w] = math.log(5/len(ClassFrequency[w]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing Term Frequencies"
      ],
      "metadata": {
        "id": "WBhQqZ8MRrnP"
      },
      "id": "WBhQqZ8MRrnP"
    },
    {
      "cell_type": "code",
      "execution_count": 359,
      "id": "9bf48ba6",
      "metadata": {
        "id": "9bf48ba6"
      },
      "outputs": [],
      "source": [
        "TermFrequency = {0: {},1: {}, 2: {}, 3: {},4: {}}\n",
        "\n",
        "for indexNo,txt in enumerate(train):\n",
        "    for w in txt.split(\" \"):\n",
        "        if w  in TermFrequency[trainLabel[indexNo]]:\n",
        "            TermFrequency[trainLabel[indexNo]][w] += 1\n",
        "        else:\n",
        "            TermFrequency[trainLabel[indexNo]][w] = 0\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 360,
      "id": "208c20ad",
      "metadata": {
        "id": "208c20ad"
      },
      "outputs": [],
      "source": [
        "dict_TF_ICF = {}\n",
        "for i in range(5):\n",
        "    dict_TF_ICF[i] = {}\n",
        "    for w in TermFrequency[i]:\n",
        "        dict_TF_ICF[i][w] = TermFrequency[i][w] * InverseClassFrequency[w]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method to get words with highest value"
      ],
      "metadata": {
        "id": "5tdsB--_TAP7"
      },
      "id": "5tdsB--_TAP7"
    },
    {
      "cell_type": "code",
      "execution_count": 361,
      "id": "82a63945",
      "metadata": {
        "id": "82a63945"
      },
      "outputs": [],
      "source": [
        "def featuresK(k):\n",
        "    feat = set()\n",
        "    for i in range(5):\n",
        "        x = list(dict(sorted(dict_TF_ICF[i].items(), key=lambda y: y[1], reverse=True)).keys())\n",
        "        feat = feat.union(set(x[:k]))\n",
        "    return feat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 362,
      "id": "98029ed2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98029ed2",
        "outputId": "31561453-4a31-4f0a-d18f-55dae4ed1b66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter no of features : 500\n"
          ]
        }
      ],
      "source": [
        "k = int(input(\"Enter no of features : \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 363,
      "id": "b254ccc6",
      "metadata": {
        "id": "b254ccc6"
      },
      "outputs": [],
      "source": [
        "vocabulary = featuresK(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 364,
      "id": "5d13305e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d13305e",
        "outputId": "db6f705c-8338-484c-a531-79061242be53"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2372"
            ]
          },
          "metadata": {},
          "execution_count": 364
        }
      ],
      "source": [
        "len(vocabulary)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method to generate new Data"
      ],
      "metadata": {
        "id": "pMzHn-VBUbo2"
      },
      "id": "pMzHn-VBUbo2"
    },
    {
      "cell_type": "code",
      "execution_count": 365,
      "id": "ae1a8445",
      "metadata": {
        "id": "ae1a8445"
      },
      "outputs": [],
      "source": [
        "def updatedDataVocab(data, vocabulary):\n",
        "    updatedData = []\n",
        "    for t in data:\n",
        "        txt = []\n",
        "        for w in t.split(\" \"):\n",
        "            if w in vocabulary:\n",
        "                txt.append(w)\n",
        "        updatedData.append(\" \".join(txt).strip())\n",
        "        \n",
        "    return updatedData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 366,
      "id": "68a34aa2",
      "metadata": {
        "id": "68a34aa2"
      },
      "outputs": [],
      "source": [
        "def getFeatures(train, test):\n",
        "    vector = TfidfVectorizer()\n",
        "    tr = vector.fit_transform(train)\n",
        "    te = vector.transform(test)\n",
        "    return tr, te"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 367,
      "id": "42d5e030",
      "metadata": {
        "id": "42d5e030"
      },
      "outputs": [],
      "source": [
        "train = updatedDataVocab(train, vocabulary)\n",
        "test = updatedDataVocab(test, vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 368,
      "id": "505dc891",
      "metadata": {
        "id": "505dc891"
      },
      "outputs": [],
      "source": [
        "(train, test) = getFeatures(train, test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 369,
      "id": "2f23e0ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f23e0ae",
        "outputId": "e102fad2-b1f3-4009-9059-19376e9a2502"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3609, 2370)"
            ]
          },
          "metadata": {},
          "execution_count": 369
        }
      ],
      "source": [
        "train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee2ac669",
      "metadata": {
        "id": "ee2ac669"
      },
      "source": [
        "### 4."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "modelNB = GaussianNB()"
      ],
      "metadata": {
        "id": "4ECOErLWZfB1"
      },
      "id": "4ECOErLWZfB1",
      "execution_count": 370,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelNB.fit(train.toarray(),trainLabel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykXTTaRRZn3l",
        "outputId": "10464c73-f5c7-4527-e703-de15d6bb76c6"
      },
      "id": "ykXTTaRRZn3l",
      "execution_count": 371,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB()"
            ]
          },
          "metadata": {},
          "execution_count": 371
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_train = modelNB.predict(train.toarray())\n",
        "predict_test = modelNB.predict(test.toarray())"
      ],
      "metadata": {
        "id": "-SAyrVDzZuAF"
      },
      "id": "-SAyrVDzZuAF",
      "execution_count": 372,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 373,
      "id": "04758b43",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04758b43",
        "outputId": "2b382092-9b5b-48f5-9203-b357d8f1c455"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.914380714879468\n",
            "0.823905558288244\n"
          ]
        }
      ],
      "source": [
        "print(sum(predict_train == trainLabel)/len(trainLabel))\n",
        "print(sum(predict_test == testLabel)/len(testLabel))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "C_RkSMfRbBuW"
      },
      "id": "C_RkSMfRbBuW",
      "execution_count": 374,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "vF3XCjP9cAs5"
      },
      "id": "vF3XCjP9cAs5",
      "execution_count": 375,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 376,
      "id": "20cb9d20",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20cb9d20",
        "outputId": "3202202c-c638-406b-bb33-bf5d54039dd3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[338,  19,  19,  36,  38],\n",
              "       [ 16, 345,  24,  32,  27],\n",
              "       [ 13,  22, 414,  12,  15],\n",
              "       [  0,   9,   9, 202,   3],\n",
              "       [ 17,   9,  21,  17, 376]])"
            ]
          },
          "metadata": {},
          "execution_count": 376
        }
      ],
      "source": [
        "confusion_matrix(testLabel,predict_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bed7d3b9",
      "metadata": {
        "id": "bed7d3b9"
      },
      "source": [
        "### 6."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0ff8d80",
      "metadata": {
        "id": "a0ff8d80"
      },
      "source": [
        "##### 50-50 Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 377,
      "id": "32aa2bda",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32aa2bda",
        "outputId": "a2863da1-a93d-4439-95ac-a68a67f9341b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy:  0.925531914893617\n",
            "Test Accuracy:  0.8096288129364204\n"
          ]
        }
      ],
      "source": [
        "trainX, trainLabel, testX, testLabel = methodRandomSplitData(0.5)\n",
        "\n",
        "trainX = updatedDataVocab(trainX, vocabulary)\n",
        "testX = updatedDataVocab(testX, vocabulary)\n",
        "\n",
        "trainX, testX = getFeatures(trainX, testX)\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(trainX.toarray(), trainLabel)\n",
        "x = gnb.predict(trainX.toarray())\n",
        "y = gnb.predict(testX.toarray())\n",
        "\n",
        "print(\"Train Accuracy: \", sum(x == trainLabel)/len(trainLabel))\n",
        "print(\"Test Accuracy: \", sum(y == testLabel)/len(testLabel))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(testLabel,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjiajnq9i6QF",
        "outputId": "c84b7625-6ea0-48f7-af33-890df4f9e747"
      },
      "id": "rjiajnq9i6QF",
      "execution_count": 378,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[460,  26,  35,  61,  38],\n",
              "       [ 38, 437,  41,  34,  24],\n",
              "       [ 14,  31, 506,  24,  26],\n",
              "       [  9,   5,  13, 273,   4],\n",
              "       [ 30,  18,  18,  29, 527]])"
            ]
          },
          "metadata": {},
          "execution_count": 378
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e105585f",
      "metadata": {
        "id": "e105585f"
      },
      "source": [
        "##### 70-30 split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 379,
      "id": "51259e17",
      "metadata": {
        "id": "51259e17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d7cab31-8a6b-41b6-ee71-5a765cd7ab63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy:  0.9211526282457252\n",
            "Test Accuracy:  0.8069151324651999\n"
          ]
        }
      ],
      "source": [
        "trainX, trainLabel, testX, testLabel = methodRandomSplitData(0.7)\n",
        "\n",
        "trainX = updatedDataVocab(trainX, vocabulary)\n",
        "testX = updatedDataVocab(testX, vocabulary)\n",
        "\n",
        "trainX, testX = getFeatures(trainX, testX)\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(trainX.toarray(), trainLabel)\n",
        "x = gnb.predict(trainX.toarray())\n",
        "y = gnb.predict(testX.toarray())\n",
        "\n",
        "print(\"Train Accuracy: \", sum(x == trainLabel)/len(trainLabel))\n",
        "print(\"Test Accuracy: \", sum(y == testLabel)/len(testLabel))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(testLabel,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btRZiNjajUeg",
        "outputId": "d10c2b76-56ef-4bb4-b18a-3975e3b3bd33"
      },
      "id": "btRZiNjajUeg",
      "execution_count": 380,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[350,  27,  21,  63,  36],\n",
              "       [ 25, 371,  42,  39,  26],\n",
              "       [ 13,  20, 426,  20,  15],\n",
              "       [  4,   1,  10, 224,   3],\n",
              "       [  9,  20,  14,  22, 426]])"
            ]
          },
          "metadata": {},
          "execution_count": 380
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "IR_Assign2_Q3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}