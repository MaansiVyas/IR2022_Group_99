{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueSKgA0TtOeB",
        "outputId": "8b8c7215-cf2b-4ecf-8c9f-2b8fded06f03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ],
      "source": [
        "import string\n",
        "import copy\n",
        "import nltk\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNZPjqMotKyV"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9iCgkaVtbRQ"
      },
      "source": [
        "Data Extraction "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPV32yny_X2U",
        "outputId": "406ddbd3-dd41-41ab-b987-59bc760216eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hMOaZ97tK4f"
      },
      "outputs": [],
      "source": [
        "title = \"Humor,Hist,Media,Food\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhuxZV3DtvKt"
      },
      "outputs": [],
      "source": [
        "paths = []\n",
        "fileNameWithId={}\n",
        "id=0\n",
        "for (dirpath, dirnames, filenames) in os.walk(str('/content/drive/MyDrive/Humor,Hist,Media,Food')):\n",
        "    for i in filenames:\n",
        "        fileNameWithId[id]=i\n",
        "        id=id+1\n",
        "        paths.append(str(dirpath)+str(\"/\")+i)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fileNameWithId"
      ],
      "metadata": {
        "id": "piiYR2Cy8Av3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7LaVBnz9BtD"
      },
      "outputs": [],
      "source": [
        "def remove_numbers(txt_data):\n",
        "   numbers=\"0123456789\"\n",
        "   for i in range(len(numbers)):\n",
        "       txt_data=np.char.replace(txt_data, numbers[i], ' ')\n",
        "       txt_data = np.char.replace(txt_data, \"  \", \" \")\n",
        "   return txt_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozLuhoX4FMio"
      },
      "outputs": [],
      "source": [
        "def punctuations_removal_from_data(txt_data):\n",
        "    sym = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
        "    for i in range(len(sym)):\n",
        "        txt_data = np.char.replace(txt_data, sym[i], ' ')\n",
        "        txt_data = np.char.replace(txt_data, \"  \", \" \")\n",
        "    txt_data = np.char.replace(txt_data, ',', ' ')\n",
        "    txt_data = np.char.replace(txt_data, \"'\", \"\")#additionally removing apostrophe\n",
        "    return txt_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDYjWX99HLjp"
      },
      "outputs": [],
      "source": [
        "def to_lower_case(txt_data):\n",
        "    return np.char.lower(txt_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FS4ezTSsHywy"
      },
      "outputs": [],
      "source": [
        "def stop_words_removal_from_data(txt_data):\n",
        "    stopWords = stopwords.words('english')\n",
        "    words = word_tokenize(str(txt_data))\n",
        "    clean_data = \"\"\n",
        "    for w in words:\n",
        "        if w not in stopWords:\n",
        "            clean_data = clean_data + \" \" + w\n",
        "    return np.char.strip(clean_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gevpVUTqIsLD"
      },
      "outputs": [],
      "source": [
        "def short_char_removal(txt_data):\n",
        "    words = word_tokenize(str(txt_data))\n",
        "    clean_data = \"\"\n",
        "    for w in words:\n",
        "        if len(w) > 1:\n",
        "            clean_data = clean_data + \" \" + w\n",
        "    return np.char.strip(clean_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-qfZWoTlY79"
      },
      "outputs": [],
      "source": [
        "def lemmatize_text(txt_data):\n",
        "    lemmatizer = WordNetLemmatizer()   \n",
        "    tokens = word_tokenize(str(txt_data))\n",
        "    clean_data = \"\"\n",
        "    for w in tokens:\n",
        "        clean_data = clean_data + \" \" +lemmatizer.lemmatize(w)\n",
        "    return np.char.strip(clean_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method for data Pre processing"
      ],
      "metadata": {
        "id": "P8CwOChPzD3N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eRCSbOsKs0G"
      },
      "outputs": [],
      "source": [
        "def process_txt_data(txt_data):\n",
        "  txt_data=remove_numbers(txt_data)\n",
        "  txt_data=punctuations_removal_from_data(txt_data)\n",
        "  txt_data=to_lower_case(txt_data)\n",
        "  txt_data=stop_words_removal_from_data(txt_data)\n",
        "  txt_data=short_char_removal(txt_data)\n",
        "  txt_data=lemmatize_text(txt_data)\n",
        "  return txt_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intersection"
      ],
      "metadata": {
        "id": "h2crGwHZzLS4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuPGJs_LD5qe"
      },
      "outputs": [],
      "source": [
        "def IntersectionCount(query,document):\n",
        "  a=set(query.tolist().split(\" \"))\n",
        "  b=set(document.tolist().split(\" \"))\n",
        "  return len(a&b)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Union"
      ],
      "metadata": {
        "id": "csa7efPzzONB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZqKhUh7GQN0"
      },
      "outputs": [],
      "source": [
        "def UnionCount(query,document):\n",
        "  a=set(query.tolist().split(\" \"))\n",
        "  b=set(document.tolist().split(\" \"))\n",
        "  return len(a|b)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method to calculate Jaccard Coeffecient"
      ],
      "metadata": {
        "id": "0hn_Zi9gzQox"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xQsF_kgGSqf"
      },
      "outputs": [],
      "source": [
        "def calculate_jaccard(query, document):\n",
        "  intersection_count_jaccard = IntersectionCount(query, document)\n",
        "  union_count_jaccard = UnionCount(query,document)\n",
        "  jaccard_coeff = intersection_count_jaccard/union_count_jaccard\n",
        "  return jaccard_coeff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8zpNTPPH443"
      },
      "outputs": [],
      "source": [
        "def find_Relevant_docs_using_jaccard_coeffecient(query):\n",
        "  text_doc_processed=[]\n",
        "  i=0\n",
        "  processed_query = process_txt_data(query)\n",
        "  jaccard_coeffecient = []\n",
        "  for path in paths:\n",
        "    file = open(path, 'r',encoding=\"cp850\")\n",
        "    text_data = file.read().strip()\n",
        "    pre_processed_text=process_txt_data(text_data)\n",
        "    jaccard_coeffecient.append(calculate_jaccard(processed_query,pre_processed_text))\n",
        "    i+=1\n",
        "    file.close()\n",
        "  jaccard_coeffecient = np.array(jaccard_coeffecient)\n",
        "  top_5_docs = jaccard_coeffecient.argsort()[-5:][::-1]\n",
        "  for i in range(len(top_5_docs)):\n",
        "    print(fileNameWithId[top_5_docs[i]])  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = input('Enter the Query   : ')\n",
        "find_Relevant_docs_using_jaccard_coeffecient(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQCTYVrKzwoq",
        "outputId": "539e80b0-1350-4f70-efd7-a1c8ee37c092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the Query   : I am Sherlock\n",
            "livnware.hum\n",
            "mov_rail.txt\n",
            "subb_lis.txt\n",
            "crzycred.lst\n",
            "insults1.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0-P8Wk1CNYt"
      },
      "source": [
        "TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPiZASJhXuaO"
      },
      "outputs": [],
      "source": [
        "doc_no = 0\n",
        "\n",
        "tokens_in_docs = []\n",
        "\n",
        "postings_list ={}\n",
        "frequency_count = {}\n",
        "\n",
        "for path in paths:\n",
        "    file = open(path, 'r', encoding= 'cp850')\n",
        "    text = \" \".join(file.read().split()) \n",
        "    file.close()\n",
        "\n",
        "    #preprocessing\n",
        "    processed_doc = process_txt_data(text)  \n",
        "    \n",
        "    #tokenization\n",
        "    generated_tokens = word_tokenize(str(processed_doc))\n",
        "    tokens_in_docs.append(generated_tokens)\n",
        "    position = 0\n",
        "\n",
        "    #postings creation\n",
        "    for token in generated_tokens:\n",
        "        if token not in postings_list:\n",
        "            postings_list[token]=[[[doc, {position}]]]\n",
        "            frequency_count[token] = 1\n",
        "            \n",
        "        else:\n",
        "            pos = postings_list[token]      \n",
        "            d = [a[0] for a in pos]\n",
        "            if doc_no in d:\n",
        "                for a in pos:\n",
        "                    if a[0] == doc:\n",
        "                        a[1].add(position)\n",
        "            else:\n",
        "                pos.append([doc,{position}])\n",
        "                frequency_count[token]+=1                           \n",
        "        position += 1\n",
        "\n",
        "    doc_no += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Raw Count Term Frequency"
      ],
      "metadata": {
        "id": "63b-0wDT9VtV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSV9qHTOJ0i5"
      },
      "outputs": [],
      "source": [
        "freq_raw_count = []   #list to store raw count term frequency\n",
        "\n",
        "#iterate in all docs \n",
        "for i in tokens_in_docs:\n",
        "    raw_term_frequency = {}      #dictionary to store raw count term frequency\n",
        "    for w in i:\n",
        "        if raw_term_frequency.get(w) != None: \n",
        "            r = raw_term_frequency.get(w)             \n",
        "            raw_term_frequency[w] = r+1                 \n",
        "        else:\n",
        "            raw_term_frequency[w] = 1 \n",
        "    freq_raw_count.append(raw_term_frequency)     "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Term Frequency"
      ],
      "metadata": {
        "id": "pIrKbMvpFz9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sum_values(list):\n",
        "  sum=0\n",
        "  for i in list:\n",
        "    sum+=i\n",
        "  return sum  "
      ],
      "metadata": {
        "id": "WkTuWXJGD-bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkF1Ax1mK_iT"
      },
      "outputs": [],
      "source": [
        "freq_term_frequency = []    #list to store term frequency variant\n",
        "for i in freq_raw_count:\n",
        "    sum = sum_values(i.values())  \n",
        "    term_freq = {}        #dictionary to store term frequency variant\n",
        "    for w in i.keys():\n",
        "        tf =(i.get(w)/sum) \n",
        "        term_freq[w] = tf\n",
        "    freq_term_frequency.append(term_freq)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Log Normalization"
      ],
      "metadata": {
        "id": "viFUDqudF2FW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riZmVF0BLCtw"
      },
      "outputs": [],
      "source": [
        "freq_log_normalization = []       #list to store log normalization term frequency\n",
        "for i in freq_raw_count:\n",
        "    log_normalization = {}         #dictionary to store log normalization term frequency\n",
        "    for l in i.keys():\n",
        "        log_normal = np.log(1+i.get(l))   \n",
        "        log_normalization[l] = log_normal    \n",
        "    freq_log_normalization.append(log_normalization)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Double Normalization"
      ],
      "metadata": {
        "id": "0fdHCqnZHHjG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lx48I-QuLFlM"
      },
      "outputs": [],
      "source": [
        "freq_double_Normalisation = []  #list to store double normalization term frequency\n",
        "for i in freq_raw_count:\n",
        "    get_max = max(i.values())\n",
        "    double_normalization = {}            #dictionary to store double normalization term frequency\n",
        "    for l in i.keys():\n",
        "        double_normalization[l] =  0.5 + (0.5*(i.get(l)/get_max))\n",
        "    freq_double_Normalisation.append(double_normalization)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Binary "
      ],
      "metadata": {
        "id": "kSJcx4f5IbPO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2cgJbEBLIHc"
      },
      "outputs": [],
      "source": [
        "freq_binary = []   #list  to store binary term frequency\n",
        "for i in freq_raw_count:\n",
        "    binary = {}         #dictionary to store binary term frequency\n",
        "    for l in i.keys():\n",
        "        freq = i.get(l)\n",
        "        if freq <= 0: \n",
        "            bool_freq = 0          \n",
        "        else:\n",
        "            bool_freq = 1  \n",
        "        binary[l] = bool_freq\n",
        "    freq_binary.append(binary)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No of docs in which a term 't' is present"
      ],
      "metadata": {
        "id": "2bFrHhgcLSWP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5to2ihMiLKyV"
      },
      "outputs": [],
      "source": [
        "\n",
        "frequency_of_term_in_all_docs = {}    #dictionary to store frequnecy for each term in the doc\n",
        "for i in tokens_in_docs:\n",
        "   for l in list(set(i)):\n",
        "        if frequency_of_term_in_all_docs.get(l) != None:                 \n",
        "            frequency_of_term_in_all_docs[l] =  frequency_of_term_in_all_docs.get(l)+1\n",
        "        else:\n",
        "            frequency_of_term_in_all_docs[l] = 1  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcUxFF9hLORR"
      },
      "source": [
        "IDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ksuSyLJLQnK"
      },
      "outputs": [],
      "source": [
        "N = len(fileName_with_Id)\n",
        "IDF = {}\n",
        "for item in frequency_of_term_in_all_docs.keys():\n",
        "    IDF[item] = np.log(N/frequency_of_term_in_all_docs[item]+1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method to calculate TF IDF values "
      ],
      "metadata": {
        "id": "72llwyYIM-3Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEvOnLyVLTse"
      },
      "outputs": [],
      "source": [
        "def compute_tfidf(list_tf):\n",
        "    list_tfidf = []  \n",
        "    for i in list_tf:\n",
        "        dict_tfidf = {}\n",
        "        for l in i.keys():\n",
        "            dict_tfidf[l] =  i[l] * IDF[l]\n",
        "        list_tfidf.append(dict_tfidf)\n",
        "    return list_tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Z-RE-JfLWYd"
      },
      "outputs": [],
      "source": [
        "#calculating tfidf for all 5 variants\n",
        "binary_tf_idf = compute_tfidf(freq_binary)\n",
        "raw_count_tf_idf = compute_tfidf(freq_raw_count)\n",
        "term_frequency_tf_idf = compute_tfidf(freq_term_frequency)\n",
        "log_normalisation_tf_idf = compute_tfidf(freq_log_normalization)\n",
        "double_normalisation_tf_idf = compute_tfidf(freq_double_Normalisation)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "List of All Terms"
      ],
      "metadata": {
        "id": "Q6sPqNuEPB5j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oP6MOYdPNb7o"
      },
      "outputs": [],
      "source": [
        "all_terms = list(frequency_of_term_in_all_docs.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TFIDF matrix using log"
      ],
      "metadata": {
        "id": "nQV1LnUyPIzt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wn3t5tdGLbGM"
      },
      "outputs": [],
      "source": [
        "log_matrix_tf_idf = []  #matrix to store tfidf values for log variant\n",
        "for i in log_normalisation_tf_idf:\n",
        "    log = []\n",
        "    for l in all_terms:\n",
        "        if l not in i :\n",
        "            log.append(0.0)  \n",
        "        else:\n",
        "            log.append(i[l])  \n",
        "            \n",
        "    log_matrix_tf_idf += [log]  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_matrix_tf_idf[0]"
      ],
      "metadata": {
        "id": "-Yhd68eu2FdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TFIDF matrix using binary"
      ],
      "metadata": {
        "id": "8t4hIylXQpep"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPO_8NLhLc5J"
      },
      "outputs": [],
      "source": [
        "binary_matrix_tf_idf = []   #matrix to store tfidf values for binary variant\n",
        "for i in binary_tf_idf:\n",
        "    bin = []\n",
        "    for l in all_terms:\n",
        "        if l not in i :\n",
        "            bin.append(0.0)   \n",
        "        else:\n",
        "            bin.append(i[l])\n",
        "           \n",
        "    binary_matrix_tf_idf += [bin]   "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TFIDF matrix using Double Noralization"
      ],
      "metadata": {
        "id": "2G1wNdx9RSBi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgtsLrejLezT"
      },
      "outputs": [],
      "source": [
        "#tf-idf score using double\n",
        "double_matrix_tf_idf = []    #matrix to store tfidf values for double normalization\n",
        "for i in double_normalisation_tf_idf:\n",
        "    db = []\n",
        "    for l in all_terms:\n",
        "        if l not in i :\n",
        "            db.append(0.0)   \n",
        "        else:\n",
        "           db.append(i[l]) \n",
        "           \n",
        "    double_matrix_tf_idf += [db]   "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TFIDF matrix using Term Frequency"
      ],
      "metadata": {
        "id": "4svX1L28SUTk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebwSPleLLguS"
      },
      "outputs": [],
      "source": [
        "term_freq_matrix_tf_idf = []   #matrix to store tfidf values for term frequency\n",
        "for i in term_frequency_tf_idf:\n",
        "    term = []\n",
        "    for l in all_terms:\n",
        "        if l not in i :\n",
        "            term.append(0.0)\n",
        "             \n",
        "        else:\n",
        "            term.append(i[l]) \n",
        "              \n",
        "    term_freq_matrix_tf_idf += [term]   "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TFIDF matrix using Raw Count"
      ],
      "metadata": {
        "id": "oRSdrj4fTG19"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja9JlW6iLgmJ"
      },
      "outputs": [],
      "source": [
        "raw_matrix_tf_idf = []   #matrix to store tfidf values for raw count\n",
        "for i in raw_count_tf_idf:\n",
        "    raw = []\n",
        "    for l in all_terms:\n",
        "        if l not in i :\n",
        "            raw.append(0.0)  \n",
        "        else:\n",
        "            raw.append(i[l])   \n",
        "    raw_matrix_tf_idf += [raw]        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4nfDe77Lkot"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "def rank_doc_using_Tf_idf_values(variant_list,list_of_queries):\n",
        "\n",
        "    vector_of_query = [0.0] * len(all_terms)\n",
        "    result = [i for i, k in enumerate(all_terms) if k in set(list_of_queries)]\n",
        "    print(result)\n",
        "\n",
        "    for r in range(len(all_terms)):\n",
        "       if r not in result:\n",
        "        vector_of_query[r]=0.0    \n",
        "       else:\n",
        "        vector_of_query[r]=1.0\n",
        "        print('Vector of Query : ',vector_of_query[r])           \n",
        "    print('Vector of Query : ',vector_of_query)\n",
        "        \n",
        "\n",
        "    rank = {}\n",
        "  \n",
        "    for p in range(N):\n",
        "        rank[p] = 0.0\n",
        "    for word in result:\n",
        "       \n",
        "        count = -1\n",
        "        for i in variant_list:\n",
        "            count += 1\n",
        "            rank[count] +=  i[word]\n",
        "       \n",
        "    rank = dict(sorted(rank.items(),key=operator.itemgetter(1),reverse=True))   \n",
        "    print(rank)\n",
        "    return rank,vector_of_query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJYyLxBaLmU4"
      },
      "outputs": [],
      "source": [
        "def top5(tfidfList,list_of_queries,k,var):\n",
        "\n",
        "    tf_list ,vectorOfQuery= rank_doc_using_Tf_idf_values(tfidfList,list_of_queries)\n",
        "    check = 0\n",
        "    result = []\n",
        "    cnt = -1    \n",
        "    for i in tf_list.keys():\n",
        "        cnt += 1\n",
        "        if cnt == k:\n",
        "            print('Top ',k,' Documents on the basis of ',var,' tf-idf values are',result)\n",
        "            for i in result:\n",
        "                print(fileName_with_Id.iloc[i][0])\n",
        "            check = 1\n",
        "            break\n",
        "        result.append(i)\n",
        "    if check == 0:\n",
        "        print('Top ',k,' Documents on the basis of ',var,' tf-idf values are:',result)    \n",
        "        for i in result:\n",
        "            print(fileName_with_Id.iloc[i][0])\n",
        "    return vectorOfQuery      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUEl9RWyONKN"
      },
      "outputs": [],
      "source": [
        "def query_tokens(query):\n",
        "    processed_query = process_txt_data(query)        #preprocessing of query\n",
        "    query_token = word_tokenize(str(processed_query))   #token generation\n",
        "    return query_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G39OktzVQI1M"
      },
      "outputs": [],
      "source": [
        "fileName_with_Id = pd.DataFrame(fileName_with_Id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3LXWyhXLpTW"
      },
      "outputs": [],
      "source": [
        "query = input('Enter the String   : ')\n",
        "#fetching all query vector\n",
        "query_afterPreprocess =query_tokens(query)  # fetching query tokens\n",
        "#fetching tfidf top 5 documents based on all 5 variants\n",
        "query_vector_raw=top5(tfidf_raw_matrix,query_afterPreprocess,5,'Raw_Count')\n",
        "query_vector_log=top5(tfidf_log_matrix,query_afterPreprocess,5,'Logarithmic')   \n",
        "query_vector_term=top5(tfidf_term_matrix,query_afterPreprocess,5,'termfrequency')\n",
        "query_vector_binary=top5(tfidf_binary_matrix,query_afterPreprocess,5,'Binary')\n",
        "query_vector_double=top5(tfidf_double_matrix,query_afterPreprocess,5,'Double') "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "IR_Assignment2_Question1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}